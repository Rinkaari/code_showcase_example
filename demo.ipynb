{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.logging import setup_logging\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config = yaml.load(open('congig.yaml', encoding='utf-8'), Loader=yaml.FullLoader)\n",
    "logger = setup_logging(name='RUN', level='DEBUG')\n",
    "\n",
    "spark = (\n",
    "  SparkSession\n",
    "  .builder\n",
    "  .appname('appname')\n",
    "  .master('yarn')\n",
    "  .config('name','value')\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import pandas as pd\n",
    "\n",
    "class Step:\n",
    "  \"\"\"\n",
    "  Базовый класс.\n",
    "  \"\"\"\n",
    "  def __init__(self, config, logger: logging.Logger, spark):\n",
    "     self.config = config\n",
    "     self.logger = logger\n",
    "     self.spark = spark\n",
    "  \n",
    "  def execute(self, data_store: pd.Dataframe):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.steps.base_class import Step\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark\n",
    "import gc\n",
    "\n",
    "\n",
    "class TargetConstructor(Step):\n",
    "    \"\"\"\n",
    "    Класс с методами по сбору целевой выборки для модели.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, logger, spark):\n",
    "        super().__init__(config = config, logger = logger, spark=spark)\n",
    "    \n",
    "    def __load_uniq_clients(self):\n",
    "        \"\"\"\n",
    "        Загружаем уникальных клиентов.\n",
    "        \"\"\"\n",
    "        self.logger.info('1.1: loading unique clients')\n",
    "\n",
    "        uniq_clients = \\\n",
    "            self.spark.sql(open(self.config['sql_paths']['uniq_clients'])\n",
    "                .read()\n",
    "                .format(self.config['dates']['date'], self.config['dates']['date']))\n",
    "        \n",
    "        uniq_clients = \\\n",
    "            uniq_clients.withColumnRenamed('start_month', 'report_date') \\\n",
    "                        .withColumn('target', lit(1))\n",
    "        \n",
    "        return uniq_clients\n",
    "    \n",
    "    def __load_mass_clients(self):\n",
    "        \"\"\"\n",
    "        Загрузка массовых клиентов.\n",
    "        \"\"\"\n",
    "        self.logger.info('1.2: loading mass clients')\n",
    "\n",
    "        mass_clients = \\\n",
    "            self.spark.sql(open(self.config['sql_paths']['mass_clients'])\n",
    "                .read()\n",
    "                .format(self.config['dates']['date'], self.config['dates']['date']))\n",
    "        \n",
    "        return mass_clients\n",
    "\n",
    "    def __union_and_write_pre_final_target(self, \n",
    "                                        uniq_clients: pyspark.sql.Dataframe, \n",
    "                                        mass_clients: pyspark.sql.Dataframe):\n",
    "        \"\"\"\n",
    "        Объединяем 0 и 1, загружаем в хранилище.\n",
    "        \"\"\"\n",
    "        self.logger.info('1.3: making target union and loading to datastore')\n",
    "\n",
    "        target_final = uniq_clients.union(mass_clients).withColumnRenamed('report_date', 'month_end_dt')\n",
    "\n",
    "        target_final.repartition(10).write.mode('overwrite').format('orc') \\\n",
    "                    .saveAsTable(self.config['temp_tables_paths']['namepath'])\n",
    "        \n",
    "        target_for_pre_selection = target_final.select('hid_party', 'month_end_dt')\n",
    "\n",
    "        return target_for_pre_selection\n",
    "\n",
    "    def __clean_workspace_after_step_1(self, \n",
    "                                       uniq_clients: pyspark.sql.Dataframe, \n",
    "                                       mass_clients: pyspark.sql.Dataframe):\n",
    "        \"\"\"\n",
    "        Подчищаем за собой.\n",
    "        \"\"\"\n",
    "        self.logger.info('1.4: clean workspace after step 1')\n",
    "\n",
    "        uniq_clients.unpersist()\n",
    "        mass_clients.unpersist()\n",
    "        gc.collect()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Запуск методов.\n",
    "        \"\"\"\n",
    "        uniq_clients = self.__load_uniq_clients()\n",
    "        mass_clients = self.__load_mass_clients()\n",
    "\n",
    "        target_for_pre_selection = self.__union_and_write_pre_final_target(uniq_clients, mass_clients)\n",
    "\n",
    "        self.__clean_workspace_after_step_1(uniq_clients, mass_clients)\n",
    "\n",
    "        return target_for_pre_selection\n",
    "\n",
    "tc = TargetConstructor(config, logger, spark)\n",
    "target_for_pre_selection = tc.execute()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
